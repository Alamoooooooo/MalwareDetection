# -*- coding: utf-8 -*-
"""
输入数据：（batch_size，IMG_W，IMG_H，col_channel）= （20,  512,  512,  3）

卷积层1： （conv_kernel，num_channel，num_out_neure）= （3,  3,  3,  64）

池化层1： （ksize，strides，padding）= （[1,3,3,1]， [1,2,2,1]， 'SAME'）

卷积层2： （conv_kernel，num_channel，num_out_neure）= （3,  3,  64,  16）

池化层2： （ksize，strides，padding）= （[1,3,3,1]， [1,1,1,1]， 'SAME'）

全连接1： （out_pool2_reshape，num_out_neure）= （dim， 128）

全连接2： （fc1_out，num_out_neure）= （128，128）

softmax层： （fc2_out，num_classes） = （128,  4）

激活函数： tf.nn.relu

损失函数： tf.nn.sparse_softmax_cross_entropy_with_logits

"""

import tensorflow as tf
#网络结构定义
    #输入参数：images，image batch、4D tensor、tf.float32、[batch_size, width, height, channels]
    #返回参数：logits, float、 [batch_size, n_classes]
    
    
def Ocnn(images, batch_size, n_classes):
#卷积+池化层x2，全连接层x2，softmax层分类

    #卷积层1
    #64个3x3的卷积核（3通道也不知道为什么3通道才有用很奇怪），padding=’SAME’，表示padding后卷积的图与原图尺寸一致，激活函数relu()
    with tf.variable_scope('conv1') as scope:
     
# tf.truncated_normal(shape, mean, stddev)这个函数产生正态分布，均值和标准差自己设定。
# shape表示生成张量的维度，mean是均值
# stddev是标准差,，默认最大为1，最小为-1，均值为0
     
        w_1 = tf.Variable(tf.truncated_normal(shape=[3,3,3,64], stddev = 1.0, dtype = tf.float32), 
                                              name = 'weights', dtype = tf.float32)
        #生成方差stddev=1的正态分布矩阵,矩阵维度shape
        ##产生一个4维矩阵,最基本的元素是3行64列,
        
 # 创建一个结构为shape矩阵也可以说是数组shape声明其行列，初始化所有值为0.1
 
        b_1 = tf.Variable(tf.constant(value = 0.1, dtype = tf.float32, shape = [64]),
                                      name = 'biases', dtype = tf.float32)
        #64个偏置值

# 卷积遍历各方向步数为1，SAME：边缘外自动补0，遍历相乘
# padding 一般只有两个值
# 卷积层后输出图像大小为：（W+2P-f）/stride+1并向下取整    
        
        conv = tf.nn.conv2d(images, w_1, strides=[1,1,1,1], padding='SAME')
        #对输入input进行卷积运算
        
        pre_activation = tf.nn.bias_add(conv, b_1)  
        #tf.nn.bias_add 是 tf.add 的一个特例:tf.add(tf.matmul(x, w), b) == tf.matmul(x, w) + b
        conv1 = tf.nn.relu(pre_activation, name= scope.name)
        #得到512*512*64图像
        
#池化层1
#3x3最大池化，步长strides为2，池化后执行lrn()操作，局部响应归一化，对训练有利。增强模型泛化能力。
# tf.nn.lrn(input,depth_radius=None,bias=None,alpha=None,beta=None,name=None)
#      input是一个4D的tensor，类型必须为float。
#       depth_radius是一个类型为int的标量，表示囊括的kernel的范围。
#       bias是偏置。
#       alpha是乘积系数，是在计算完囊括范围内的kernel的激活值之和之后再对其进行乘积。
#       beta是指数系数。
#LRN是normalization的一种，normalizaiton的目的是抑制，抑制神经元的输出。而LRN的设计借鉴了神经生物学中的一个概念，叫做“侧抑制”。

    # 池化卷积结果（conv2d）池化层采用kernel大小为3*3，步数也为2，SAME：周围补0，取最大值。数据量缩小了4倍
    # x 是 CNN 第一步卷积的输出量，其shape必须为[batch, height, weight, channels];
    # ksize 是池化窗口的大小， shape为[batch, height, weight, channels]
    # stride 步长，一般是[1，stride， stride，1]
    # 池化层输出图像的大小为(W-f)/stride+1，向上取整

    with tf.variable_scope('pooling1_lrn') as scope:
        pool1 = tf.nn.max_pool(conv1, ksize=[1,3,3,1],strides=[1,2,2,1],padding='SAME', name='pooling1')
        norm1 = tf.nn.lrn(pool1, depth_radius=4, bias=1.0, alpha=0.001/9.0, beta=0.75, name='norm1')
 
#卷积层2
#32个3x3的卷积核（16通道），padding=’SAME’，表示padding后卷积的图与原图尺寸一致，激活函数relu()
    with tf.variable_scope('conv2') as scope:
        w_2 = tf.Variable(tf.truncated_normal(shape=[3,3,64,32], stddev = 0.1, dtype = tf.float32), 
                              name = 'weights', dtype = tf.float32)
        
        b_2 = tf.Variable(tf.constant(value = 0.1, dtype = tf.float32, shape = [32]),
                             name = 'biases', dtype = tf.float32)
        #32个偏置值
        conv = tf.nn.conv2d(norm1, w_2, strides = [1,1,1,1],padding='SAME')
        pre_activation = tf.nn.bias_add(conv, b_2)
        conv2 = tf.nn.relu(pre_activation, name='conv2')
        
 
#池化层2
#3x3最大池化，步长strides为2，池化后执行lrn()操作，
    with tf.variable_scope('pooling2_lrn') as scope:
        norm2 = tf.nn.lrn(conv2, depth_radius=4, bias=1.0, alpha=0.001/9.0,beta=0.75,name='norm2')
        pool2 = tf.nn.max_pool(norm2, ksize=[1,3,3,1], strides=[1,1,1,1],padding='SAME',name='pooling2')
        
        
#全连接层3
#128个神经元，将之前pool层的输出reshape成一行，激活函数relu()
    with tf.variable_scope('local3') as scope:
        #reshape = tf.reshape(norm2, shape=[batch_size, -1])
        reshape = tf.reshape(pool2, shape=[batch_size, -1])
        dim = reshape.get_shape()[1].value
        w_fc1 = tf.Variable(tf.truncated_normal(shape=[dim,128], stddev = 0.005, dtype = tf.float32),
                             name = 'weights', dtype = tf.float32)
        
        b_fc1 = tf.Variable(tf.constant(value = 0.1, dtype = tf.float32, shape = [128]), 
                             name = 'biases', dtype=tf.float32)
        
        l_fc1 = tf.nn.relu(tf.matmul(reshape, w_fc1) + b_fc1, name=scope.name)

#全连接层4
#128个神经元，激活函数relu() 
    with tf.variable_scope('local4') as scope:
        w_fc2 = tf.Variable(tf.truncated_normal(shape=[128,128], stddev = 0.005, dtype = tf.float32),
                              name = 'weights',dtype = tf.float32)
        
        b_fc2 = tf.Variable(tf.constant(value = 0.1, dtype = tf.float32, shape = [128]),
                             name = 'biases', dtype = tf.float32)
        
        l_fc2 = tf.nn.relu(tf.matmul(l_fc1, w_fc2) + b_fc2, name=scope.name)
 
#dropout层   
    fc2_dropout = tf.nn.dropout(l_fc2,0.8)
     # tf.nn.dropout(x, keep_prob, noise_shape=None, seed=None, name=None)
#    with tf.variable_scope('dropout') as scope:
#        drop_out = tf.nn.dropout(local4, 0.8)
     
        
#Softmax回归层
#将前面的全连接层输出，做一个线性回归，计算出每一类的得分，在这里是2类，所以这个层输出的是两个得分。
    with tf.variable_scope('softmax_linear') as scope:
        weights = tf.Variable(tf.truncated_normal(shape=[128, n_classes], stddev = 0.005, dtype = tf.float32),
                              name = 'softmax_linear', dtype = tf.float32)
        
        biases = tf.Variable(tf.constant(value = 0.1, dtype = tf.float32, shape = [n_classes]),
                             name = 'biases', dtype = tf.float32)
        
        softmax_linear = tf.add(tf.matmul(fc2_dropout, weights), biases, name='softmax_linear')
 
    return softmax_linear
#返回softmax层的输出
 
#loss计算
    #传入参数：logits，网络计算输出值。labels，真实值，在这里是0或者1
    #返回参数：loss，损失值
def losses(logits, labels):
    with tf.variable_scope('loss') as scope:
        cross_entropy =tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels, name='xentropy_per_example')
        loss = tf.reduce_mean(cross_entropy, name='loss')
        tf.summary.scalar(scope.name+'/loss', loss)
    return loss
 
#loss损失值优化
    #输入参数：loss。learning_rate，学习速率。
    #返回参数：train_op，训练op，这个参数要输入sess.run中让模型去训练。
def trainning(loss, learning_rate):
    with tf.name_scope('optimizer'):
        optimizer = tf.train.AdamOptimizer(learning_rate= learning_rate)
        global_step = tf.Variable(0, name='global_step', trainable=False)
        train_op = optimizer.minimize(loss, global_step= global_step)
    return train_op
 

#评价/准确率计算
    #输入参数：logits，网络计算值。labels，标签，也就是真实值，在这里是0或者1。
    #返回参数：accuracy，当前step的平均准确率，也就是在这些batch中多少张图片被正确分类了。
def evaluation(logits, labels):
    with tf.variable_scope('accuracy') as scope:
        correct = tf.nn.in_top_k(logits, labels, 1)
        correct = tf.cast(correct, tf.float16)
        accuracy = tf.reduce_mean(correct)
        tf.summary.scalar(scope.name+'/accuracy', accuracy)
    return accuracy
 

'''
#全连接层3
#128个神经元，将之前pool层的输出reshape成一行，激活函数relu()
    with tf.variable_scope('local3') as scope:
        reshape = tf.reshape(pool2, shape=[batch_size, -1])
        dim = reshape.get_shape()[1].value
        weights = tf.Variable(tf.truncated_normal(shape=[dim,128], stddev = 0.005, dtype = tf.float32),
                             name = 'weights', dtype = tf.float32)
        
        biases = tf.Variable(tf.constant(value = 0.1, dtype = tf.float32, shape = [128]), 
                             name = 'biases', dtype=tf.float32)
        
        local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)
'''


        
